Ø§ÛŒÙ† Ø³Ù†Ø¯ Ø¬Ø§Ù…Ø¹ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ùˆ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ… Â«Ø¯Ø±ÛŒÚ†Ù‡ Ø¯Ø§Ø¯Ù‡ Ù¾Ø³ØªÚ¯Ø±Ø³Â» (PostgreSQL Data Gateway) Ø¨Ø± Ø§Ø³Ø§Ø³ Ú†Ø§Ø±Ú†ÙˆØ¨ **TOGAF** ØªØ¯ÙˆÛŒÙ† Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø§ÛŒÙ† Ø³Ù†Ø¯ Ø´Ø§Ù…Ù„ ØªÙ…Ø§Ù…ÛŒ Ù…Ø±Ø§Ø­Ù„ Ø§Ø² Ø¯ÛŒØ¯ ØªØ¬Ø§Ø±ÛŒ ØªØ§ ÙÙ†ÛŒØŒ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ØŒ Ùˆ Ú©Ø¯Ù‡Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù‚Ø§Ø¨Ù„ Ú©Ù¾ÛŒâ€ŒÙ¾ÛŒØ³Øª Ø§Ø³Øª.

---

# Ø³Ù†Ø¯ Ù…Ø¹Ù…Ø§Ø±ÛŒä¼ä¸šä¸ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ (TOGAF ADM)

## ÙØ§Ø² A: Ú†Ø´Ù…â€ŒØ§Ù†Ø¯Ø§Ø² Ù…Ø¹Ù…Ø§Ø±ÛŒ (Architecture Vision)

### Û±. Ù…Ù‚Ø¯Ù…Ù‡ Ùˆ Ø§Ù‡Ø¯Ø§Ù
Ù‡Ø¯Ù Ø§Ø² Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© **Ø¯Ø±ÛŒÚ†Ù‡ Ø¯Ø§Ø¯Ù‡ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ (Data Gateway)** Ø§Ø³Øª Ú©Ù‡ Ø§Ù…Ú©Ø§Ù† Ø¯Ø³ØªØ±Ø³ÛŒ Ø§Ù…Ù†ØŒ Ù…Ø¯ÛŒØ±ÛŒØªâ€ŒØ´Ø¯Ù‡ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø§Ú©Ù†Ø¯Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³â€ŒÙ‡Ø§ÛŒ SQL Server Ùˆ MySQL Ø±Ø§ Ø§Ø² Ø·Ø±ÛŒÙ‚ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ù…Ø±Ú©Ø²ÛŒ PostgreSQL ÙØ±Ø§Ù‡Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

**Ø§Ù‡Ø¯Ø§Ù Ú©Ù„ÛŒØ¯ÛŒ:**
*   **ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:** Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø¬Ø¯Ø§ÙˆÙ„ Ø®Ø§Ø±Ø¬ÛŒ Ù…Ø§Ù†Ù†Ø¯ Ø¬Ø¯Ø§ÙˆÙ„ Ù…Ø­Ù„ÛŒ.
*   **Ø§Ù…Ù†ÛŒØª Ù…Ø±Ú©Ø²ÛŒ:** Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø³ØªØ±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ø³Ø·Ø­ Ù¾Ø³ØªÚ¯Ø±Ø³ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø¨Ø¹.
*   **Ù…Ø§Ú˜ÙˆÙ„Ø§Ø± Ø¨ÙˆØ¯Ù†:** Ø§Ù…Ú©Ø§Ù† Ø§ÙØ²ÙˆØ¯Ù† Ø¯ÛŒØªØ§Ø¨ÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ ØªÙ†Ù‡Ø§ Ø¨Ø§ ÙˆÛŒØ±Ø§ÛŒØ´ ÛŒÚ© ÙØ§ÛŒÙ„ YAML.
*   **Ú©Ø§Ø±Ø§ÛŒÛŒ:** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙˆØ§Ø¨Ø¹ Ø§ÙØ²Ø§ÛŒØ´ÛŒ (Incremental Functions) Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø¬ÛŒÙ….

### Û². Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ø·Ø­ Ø¨Ø§Ù„Ø§ (High-Level Architecture)

```mermaid
graph TD
    Client[Ú©Ø§Ø±Ø¨Ø±Ø§Ù†/Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù†â€ŒÙ‡Ø§] --> PG[PostgreSQL Gateway]
    
    subgraph "Gateway Container"
        PG
        PyScript[Python Provisioning Engine]
        Conf[Config Files YAML]
    end
    
    PG -- "FDW (tds_fdw)" --> MSSQL[(SQL Server)]
    PG -- "FDW (mysql_fdw)" --> MYSQL[(MySQL)]
    
    PyScript -.->|Read Config| Conf
    PyScript -.->|Create FDW & Functions| PG
```

---

## ÙØ§Ø² B: Ù…Ø¹Ù…Ø§Ø±ÛŒ Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø± (Business Architecture)

### Û±. Ø¬Ø±ÛŒØ§Ù† Ø§Ø±Ø²Ø´ (Value Stream)
1.  **Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¯Ø³ØªØ±Ø³ÛŒ:** Ø§Ø¯Ù…ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ ÛŒÚ© Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ YAML Ø«Ø¨Øª Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
2.  **ØªØ§Ù…ÛŒÙ† Ø®ÙˆØ¯Ú©Ø§Ø±:** Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù¾Ø§ÛŒØªÙˆÙ† ØªØºÛŒÛŒØ±Ø§Øª Ø±Ø§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ùˆ Ø²ÛŒØ±Ø³Ø§Ø®Øª (FDW, Users) Ø±Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
3.  **Ø¯Ø³ØªØ±Ø³ÛŒ Ø§Ù…Ù†:** Ú©Ø§Ø±Ø¨Ø± Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Ø·Ø±ÛŒÙ‚ ØªÙˆØ§Ø¨Ø¹ ÛŒØ§ Ø¬Ø¯Ø§ÙˆÙ„ Ù…Ø¬Ø§Ø²ÛŒ Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¯Ø§Ø±Ø¯ Ø¨Ø¯ÙˆÙ† Ø§ÛŒÙ†Ú©Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø¨Ù‡ Ù…Ù†Ø¨Ø¹ ÙˆØµÙ„ Ø´ÙˆØ¯.

### Û². Ù†Ù…ÙˆØ¯Ø§Ø± ÙØ±Ø¢ÛŒÙ†Ø¯ Provisioning

```mermaid
sequenceDiagram
    participant Dev as Ø§Ø¯Ù…ÛŒÙ†
    participant Git as Git Repo (YAML)
    participant Script as Python Engine
    participant DB as PostgreSQL
    participant Ext as External DB
    
    Dev->>Git: Commit new domain.yaml
    Script->>Git: Pull Changes
    Script->>DB: Connect as Admin
    loop For each Domain
        Script->>DB: Create Database & Schema
        Script->>DB: Create Extension (tds_fdw)
        Script->>DB: Create Server & User Mapping
        Script->>Ext: Test Connection
        Script->>DB: Create Foreign Tables
        Script->>DB: Create Views & Functions
        Script->>DB: Grant Permissions
    end
    Script-->>Dev: Report Success/Failure
```

---

## ÙØ§Ø² C: Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ (Information Systems Architecture)

### Û±. Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ (Data Architecture)
Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ø¬Ø§Ø²ÛŒ (Virtual) Ø¯Ø± Ù¾Ø³ØªÚ¯Ø±Ø³ Ù…Ø¯ÛŒØ±ÛŒØª Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. Ù‡ÛŒÚ† Ú©Ù¾ÛŒ ÙÛŒØ²ÛŒÚ©ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ Ù…Ú¯Ø± Ø§ÛŒÙ†Ú©Ù‡ Ø¨ØµÙˆØ±Øª Materialized View ØªØ¹Ø±ÛŒÙ Ø´ÙˆØ¯.

**Ø³Ø§Ø®ØªØ§Ø± Ù†Ø§Ù…â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¯Ø§ÛŒÙ†Ø§Ù…ÛŒÚ©:**
*   Ù…ØªØºÛŒØ± `__parent__`: Ù†Ø§Ù… Ù¾ÙˆØ´Ù‡ ÙˆØ§Ù„Ø¯ (Ù…Ø«Ù„Ø§Ù‹ `billing`).
*   Ù…ØªØºÛŒØ± `__current__`: Ù†Ø§Ù… Ù¾ÙˆØ´Ù‡ Ø¬Ø§Ø±ÛŒ (Ù…Ø«Ù„Ø§Ù‹ `alborz`).
*   Ù†ØªÛŒØ¬Ù‡: `billing_alborz_db`.

### Û². Ù…Ø¹Ù…Ø§Ø±ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ (Application Architecture)
**Ù…ÙˆØªÙˆØ± Provisioning (Python):**
Ø§ÛŒÙ† Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù…ØºØ² Ù…ØªÙÚ©Ø± Ø³ÛŒØ³ØªÙ… Ø§Ø³Øª. ÙˆØ¸Ø§ÛŒÙ Ø¢Ù†:
1.  Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ YAML.
2.  ØªØ´Ø®ÛŒØµ ØªØºÛŒÛŒØ±Ø§Øª.
3.  ØªÙˆÙ„ÛŒØ¯ Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø³ØªÙˆØ±Ø§Øª DDL (Data Definition Language).

**Ù†Ù…ÙˆØ¯Ø§Ø± Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øª:**

```mermaid
graph LR
    A[YAML Files] --> B(Parser)
    B --> C[Logic Processor]
    C --> D[SQL Generator]
    D --> E[Executor]
    E --> F[(PostgreSQL)]
    
    subgraph "Python Script"
        B
        C
        D
        E
    end
```

---

## ÙØ§Ø² D: Ù…Ø¹Ù…Ø§Ø±ÛŒ ÙÙ†Ø§ÙˆØ±ÛŒ (Technology Architecture)

### Û±. Ù¾Ø´ØªÙ‡ ÙÙ†Ø§ÙˆØ±ÛŒ (Technology Stack)
*   **Ø²ÛŒØ±Ø³Ø§Ø®Øª:** Docker / Docker Compose.
*   **Ø¯ÛŒØªØ§Ø¨ÛŒØ³:** PostgreSQL 18 (Ø¨Ø±Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø¨Ù‡ØªØ± Ø§Ø² PL/pgSQL Ùˆ FDW).
*   **Ø¯Ø±Ø§ÛŒÙˆØ±Ù‡Ø§:** `tds_fdw` (Ø¨Ø±Ø§ÛŒ SQL Server)ØŒ `mysql_fdw`.
*   **Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ú©Ù…Ú©ÛŒ:** FreeTDS (Ù†Ø³Ø®Ù‡ 1.4.7).

### Û². Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ FDW (Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ú©Ø§Ù…Ù„ Foreign Data Wrapper)

**Ø§Ù„Ù) Ù…ÙÙ‡ÙˆÙ… FDW:**
FDW ÛŒØ§ Ø¨Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ÛŒØŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ÛŒ Ø¯Ø± SQL Ø§Ø³Øª Ú©Ù‡ Ø¨Ù‡ Ù¾Ø³ØªÚ¯Ø±Ø³ Ø§Ø¬Ø§Ø²Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ø±Ø§ Ù…Ø§Ù†Ù†Ø¯ Ø¬Ø¯Ø§ÙˆÙ„ Ø®ÙˆØ¯Ø´ Ú©ÙˆØ¦Ø±ÛŒ Ø¨Ø²Ù†Ø¯.

**Ø¨) Ø¬Ø±ÛŒØ§Ù† Ø¯Ø§Ø¯Ù‡ Ø¯Ø± FDW:**

```mermaid
graph LR
    Q[SQL Query] --> PG[PostgreSQL Planner]
    PG --> FDW[FDW Handler]
    FDW -- "Generate Remote SQL" --> LIB[Library (FreeTDS/ODBC)]
    LIB -- "TDS Protocol" --> MSSQL[(SQL Server)]
    MSSQL -- "Data Rows" --> FDW
    FDW -- "Return Rows" --> PG
```

**Ø¬) Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø­ÛŒØ§ØªÛŒ (FreeTDS):**
Ø¨Ø±Ø§ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ SQL ServerØŒ ÙØ§ÛŒÙ„ `freetds.conf` Ù†Ù‚Ø´ ÙˆØ§Ø³Ø· Ø±Ø§ Ø¯Ø§Ø±Ø¯. ØªÙ†Ø¸ÛŒÙ… Ù†Ø§Ø¯Ø±Ø³Øª Ø¢Ù† Ù…Ù†Ø¬Ø± Ø¨Ù‡ Ø®Ø·Ø§ÛŒ `TDS_DEAD` ÛŒØ§ Ù‚Ø·Ø¹ Ø§ØªØµØ§Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

---

## ÙØ§Ø² E & G: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø¹Ù…Ù„ÛŒØ§ØªÛŒ (Implementation & Governance)

Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ ØªÙ…Ø§Ù…ÛŒ Ú©Ø¯Ù‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ù„Ø§ Ø¢ÙˆØ±Ø¯Ù† Ø³ÛŒØ³ØªÙ… Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ø§Ù…Ù„ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª.

### Û±. Ø³Ø§Ø®ØªØ§Ø± Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ (Directory Structure)
```text
project_root/
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ provision.py
â”‚   â”œâ”€â”€ init-db.sh
â”‚   â””â”€â”€ backup.sh
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ freetds.conf
â”‚   â””â”€â”€ domains/
â”‚       â”œâ”€â”€ billing/
â”‚       â”‚   â””â”€â”€ 26/
â”‚       â”‚       â””â”€â”€ db_hot26.yaml
â”‚       â””â”€â”€ brc/
â”‚           â””â”€â”€ domain.yaml
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ .env
```

### Û². ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯

#### ÙØ§ÛŒÙ„ `configs/freetds.conf`
Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø§ØªØµØ§Ù„ Ù¾Ø§ÛŒØ¯Ø§Ø± tds_fdw Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø³Øª.

```ini
[global]
    # Ù†Ø³Ø®Ù‡ Ù¾Ø±ÙˆØªÚ©Ù„ Ø¨Ø±Ø§ÛŒ SQL Server 2012 Ø¨Ù‡ Ø¨Ø§Ù„Ø§
    tds version = 7.4
    client charset = UTF-8
    
    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØªØ§ÛŒÙ…â€ŒØ§ÙˆØª Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù‚Ø·Ø¹ Ø´Ø¯Ù†
    connect timeout = 60
    query timeout = 600
    
    # Keep-Alive Ø¨Ø±Ø§ÛŒ Ø§ØªØµØ§Ù„Ø§Øª Ø·ÙˆÙ„Ø§Ù†ÛŒ
    keep alive = 1
    keep alive idle = 60
    keep alive interval = 10
    keep alive count = 5
    
    text size = 64512

# ØªØ¹Ø±ÛŒÙ Ø¨Ø®Ø´ Ø§Ø®ØªØµØ§ØµÛŒ (Ù…Ø«Ø§Ù„)
[BILLING_26]
    host = ${BILLING_DB_HOST_26}
    port = ${BILLING_DB_PORT}
    tds version = 7.4
```

#### ÙØ§ÛŒÙ„ Ù†Ù…ÙˆÙ†Ù‡ `configs/domains/billing/26/db_hot26.yaml`
Ø§ÛŒÙ† ÙØ§ÛŒÙ„ ØªØ¹Ø±ÛŒÙâ€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ùˆ ØªÙˆØ§Ø¨Ø¹ Ø§Ø³Øª.

```yaml
database:
  name: ${__parent__}_${__current__}_db 

fdws:
  - name: hot26_alborz_link_local
    type: tds
    config_section: BILLING_26
    user: ${BILLING_DB_USER}
    password: ${BILLING_DB_PASSWORD}

schemas:
  - name: hot_26  

tables:
  - name: hot_26.billparts
    type: foreign
    server: hot26_alborz_link_local
    remote_schema: dbo
    remote_table: billparts
    columns:           
      - {name: _billid, type: bigint}
      - {name: persianyear, type: smallint}
      - {name: persianmonth, type: smallint}
      # Ø³Ø§ÛŒØ± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§...

# ØªØ¹Ø±ÛŒÙ ØªØ§Ø¨Ø¹ Ø³ÙØ§Ø±Ø´ÛŒ
custom_functions:
  - name: hot_26.get_billparts_stats
    target_table: hot_26.billparts
    return_columns:
      - name: total_count
        type: bigint
        expression: count(*)
      - name: min_id
        type: bigint
        expression: min(_billid)
    filter_columns:         
      - name: persianyear
        type: int
      - name: persianmonth
        type: int
    allowed_consumers:
      - ${PENDAR_ETL_USER}
```

### Û³. Ø¯Ø§Ú©Ø± ÙØ§ÛŒÙ„ (`docker/Dockerfile`)

```dockerfile
############################################
# Stage 1 â€“ Builder (FDWs + FreeTDS)
############################################
FROM postgres:18.1-bookworm AS builder

ENV DEBIAN_FRONTEND=noninteractive \
    TZ=Asia/Tehran \
    FREETDS_VERSION=1.4.7

WORKDIR /build

# Debian sources
RUN rm -f /etc/apt/sources.list /etc/apt/sources.list.d/* && \
    printf "deb http://mirror.arvancloud.ir/debian bookworm main contrib non-free\n\
deb http://mirror.arvancloud.ir/debian-security bookworm-security main contrib non-free\n\
deb http://mirror.arvancloud.ir/debian bookworm-updates main contrib non-free\n" \
    > /etc/apt/sources.list

# Build tools
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential git wget gnupg2 dirmngr lsb-release \
    autoconf automake libtool pkg-config libssl-dev \
    libiconv-hook-dev unixodbc-dev default-libmysqlclient-dev ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# PostgreSQL dev package
RUN wget -qO- https://www.postgresql.org/media/keys/ACCC4CF8.asc | gpg --dearmor > /usr/share/keyrings/pgdg.gpg
RUN echo "deb [signed-by=/usr/share/keyrings/pgdg.gpg] http://apt.postgresql.org/pub/repos/apt bookworm-pgdg main" > /etc/apt/sources.list.d/pgdg.list
RUN apt-get update && apt-get install -y --no-install-recommends postgresql-server-dev-18 && rm -rf /var/lib/apt/lists/*

# Build FreeTDS
RUN wget https://www.freetds.org/files/stable/freetds-${FREETDS_VERSION}.tar.gz && \
    tar -xzf freetds-${FREETDS_VERSION}.tar.gz && \
    cd freetds-${FREETDS_VERSION} && \
    ./configure --prefix=/usr/local --with-openssl --enable-msdblib --enable-sybase-compat --disable-odbc && \
    make -j$(nproc) && make install && ldconfig

# Build mysql_fdw
RUN git clone --depth 1 https://github.com/EnterpriseDB/mysql_fdw.git && \
    cd mysql_fdw && make USE_PGXS=1 && make USE_PGXS=1 install

# Build tds_fdw
RUN git clone --depth 1 https://github.com/tds-fdw/tds_fdw.git && \
    cd tds_fdw && make USE_PGXS=1 FREETDS_CONFIG=/usr/local/bin/freetds-config && make USE_PGXS=1 install

############################################
# Stage 2 â€“ Runtime PostgreSQL
############################################
FROM postgres:18.1-bookworm

ENV DEBIAN_FRONTEND=noninteractive TZ=Asia/Tehran

WORKDIR /app

# Runtime dependencies
RUN rm -f /etc/apt/sources.list /etc/apt/sources.list.d/* && \
    printf "deb http://mirror.arvancloud.ir/debian bookworm main contrib non-free\n\
deb http://mirror.arvancloud.ir/debian-security bookworm-security main contrib non-free\n\
deb http://mirror.arvancloud.ir/debian bookworm-updates main contrib non-free\n" > /etc/apt/sources.list

RUN apt-get update && apt-get install -y --no-install-recommends \
    ca-certificates tzdata openssl cron python3 python3-pip wget gnupg2 \
    unixodbc libssl3 libiconv-hook1 default-libmysqlclient-dev \
    && rm -rf /var/lib/apt/lists/*

# Install Microsoft ODBC Driver 18
RUN wget -qO- https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > /usr/share/keyrings/microsoft-prod.gpg
RUN echo "deb [signed-by=/usr/share/keyrings/microsoft-prod.gpg] https://packages.microsoft.com/debian/12/prod bookworm main" > /etc/apt/sources.list.d/microsoft-prod.list
RUN apt-get update && ACCEPT_EULA=Y apt-get install -y --no-install-recommends msodbcsql18 && rm -rf /var/lib/apt/lists/*

# Copy FDWs + FreeTDS from builder
COPY --from=builder /usr/local /usr/local
COPY --from=builder /usr/lib/postgresql/18/lib/*mysql* /usr/lib/postgresql/18/lib/
COPY --from=builder /usr/lib/postgresql/18/lib/*tds* /usr/lib/postgresql/18/lib/
COPY --from=builder /usr/share/postgresql/18/extension/*mysql* /usr/share/postgresql/18/extension/
COPY --from=builder /usr/share/postgresql/18/extension/*tds* /usr/share/postgresql/18/extension/
RUN ldconfig

# FreeTDS config
RUN mkdir -p /etc/freetds
COPY ./configs/freetds.conf /etc/freetds/freetds.conf
ENV FREETDSCONF=/etc/freetds/freetds.conf

# Python packages
RUN pip3 install --break-system-packages psycopg2-binary pyyaml -i https://pypi.tuna.tsinghua.edu.cn/simple

# App scripts
COPY scripts ./scripts
COPY configs ./configs
RUN chmod +x ./scripts/*.sh && touch /var/log/backup.log

# Cron
RUN echo "0 2 * * * /app/scripts/backup.sh >> /var/log/backup.log 2>&1" > /etc/cron.d/pg-backup
RUN chmod 0644 /etc/cron.d/pg-backup && crontab /etc/cron.d/pg-backup

# Healthcheck
HEALTHCHECK --interval=30s --timeout=5s --start-period=30s CMD pg_isready -U $POSTGRES_USER -d $POSTGRES_DB || exit 1

CMD []
```

### Û´. Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§ÛŒ Ø´Ø±ÙˆØ¹ Ùˆ Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ

#### `scripts/init-db.sh`
```bash
#!/bin/bash
set -e

echo "ğŸš€ Starting PostgreSQL Gateway Initialization..."

PYTHON_SCRIPT_PATH="/app/scripts/provision.py"

# 1. Start Cron
echo "ğŸ•’ Starting Cron Service..."
service cron start

# 2. Ensure socket directory
mkdir -p /var/run/postgresql
chown postgres:postgres /var/run/postgresql
chmod 775 /var/run/postgresql

# 3. Start PostgreSQL in background
echo "â³ Starting PostgreSQL in background..."
docker-entrypoint.sh postgres &
POSTGRES_PID=$!

# 4. Wait for PostgreSQL
sleep 2
echo "â³ Waiting for PostgreSQL to accept connections..."
until pg_isready -U "$POSTGRES_USER" -d "$POSTGRES_DB"; do
  echo "Waiting for database..."
  sleep 1
done
echo "âœ… PostgreSQL is ready."

# 5. Run Python Provisioning
if [ -f "$PYTHON_SCRIPT_PATH" ]; then
    echo "ğŸ Running Python initialization script..."
    gosu postgres python3 "$PYTHON_SCRIPT_PATH"
    echo "âœ… Python script executed successfully."
else
    echo "âš ï¸  Python script not found. Skipping."
fi

# 6. Stop temporary PostgreSQL
echo "ğŸ›‘ Stopping temporary PostgreSQL process..."
gosu postgres pg_ctl -D "$PGDATA" -m fast stop
wait $POSTGRES_PID 2>/dev/null || true

# 7. Start Final PostgreSQL
echo "ğŸš€ Starting PostgreSQL in foreground mode..."
exec docker-entrypoint.sh postgres
```

#### `scripts/backup.sh`
```bash
#!/bin/bash
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="${BACKUP_DIR}/full_backup_${TIMESTAMP}.dump"

echo "ğŸ“¦ Starting backup to ${BACKUP_FILE}..."
mkdir -p "${BACKUP_DIR}"

PGPASSWORD="${POSTGRES_PASSWORD}" pg_dump \
  -h localhost -U "${POSTGRES_USER}" -d "${POSTGRES_DB}" \
  -F c -f "${BACKUP_FILE}"

if [ $? -eq 0 ]; then
  echo "âœ… Backup successful: ${BACKUP_FILE}"
  echo "ğŸ§¹ Cleaning old backups..."
  find "${BACKUP_DIR}" -type f -name "*.dump" -mtime +${BACKUP_RETENTION_DAYS} -delete
else
  echo "âŒ Backup failed!"
fi
```

### Ûµ. Ù…ÙˆØªÙˆØ± Provisioning Ù¾Ø§ÛŒØªÙˆÙ† (Ú©Ø§Ù…Ù„ Ùˆ Ø¨Ø¯ÙˆÙ† Ø­Ø°Ù)

Ø§ÛŒÙ† Ú©Ø¯ Ø´Ø§Ù…Ù„ ØªÙ…Ø§Ù…ÛŒ Ø§ØµÙ„Ø§Ø­Ø§Øª Ø§Ø¹Ù…Ø§Ù„ Ø´Ø¯Ù‡ (Static SQLØŒ Ø±ÙØ¹ Ø®Ø·Ø§Ù‡Ø§ÛŒ FDWØŒ Ù…Ø¯ÛŒØ±ÛŒØª ØªÙˆØ§Ø¨Ø¹) Ø§Ø³Øª.

**ÙØ§ÛŒÙ„: `scripts/provision.py`**

```python
import os
import psycopg2
from psycopg2 import sql
import yaml
from pathlib import Path
from collections import defaultdict

# ==========================================
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³Ø±Ø§Ø³Ø±ÛŒ
# ==========================================
DB_ADMIN_USER = os.environ.get('POSTGRES_USER')
DB_ADMIN_PASS = os.environ.get('POSTGRES_PASSWORD')
DB_DEFAULT_NAME = os.environ.get('POSTGRES_DB')

def connect(db='postgres'):
    try:
        conn = psycopg2.connect(
            host='localhost', port='5432',
            user=DB_ADMIN_USER,
            password=DB_ADMIN_PASS,
            database=db
        )
        conn.autocommit = True
        return conn
    except Exception as e:
        print(f"âŒ Connection failed to {db}: {e}")
        raise

def resolve_global_env(val):
    if not isinstance(val, str): return val
    if val.startswith('${') and val.endswith('}'):
        env_key = val[2:-1]
        resolved = os.environ.get(env_key)
        if not resolved: return val
        return resolved
    return val

def resolve_config_val(val, context_vars):
    if not isinstance(val, str): return val
    for key, value in context_vars.items():
        val = val.replace(f"${{{key}}}", value)
    if val.startswith('${') and val.endswith('}'):
        env_key = val[2:-1]
        resolved = os.environ.get(env_key, val)
        return resolved
    return val

def parse_identifier(name_str):
    if not name_str: return None
    parts = name_str.split('.')
    return sql.Identifier(*parts)

# ==========================================
# Ø²ÛŒØ±Ø³Ø§Ø®Øª Ø§Ù…Ù†ÛŒØªÛŒ
# ==========================================

def setup_security_infrastructure(conn_db):
    cur = conn_db.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS auth_policies (
            username VARCHAR(255) PRIMARY KEY, 
            allowed_start TIME, 
            allowed_end TIME, 
            description TEXT
        );
    """)
    cur.execute("""
        CREATE OR REPLACE FUNCTION enforce_access_policy() RETURNS VOID AS $$
        DECLARE rec RECORD; 
        BEGIN
            SELECT * INTO rec FROM auth_policies WHERE username = current_user;
            IF FOUND AND rec.allowed_start IS NOT NULL AND rec.allowed_end IS NOT NULL THEN
                IF current_time < rec.allowed_start OR current_time > rec.allowed_end THEN
                    RAISE EXCEPTION 'Access DENIED for % outside allowed hours', current_user;
                END IF;
            END IF;
        END; $$ LANGUAGE plpgsql;
    """)
    cur.close()

# ==========================================
# ØªÙˆØ§Ø¨Ø¹ Ø§ÙØ²Ø§ÛŒØ´ÛŒ (Incremental Jobs)
# ==========================================

def create_incremental_job_functions(conn_db, jobs_config, context_vars):
    cur = conn_db.cursor()
    for job in jobs_config:
        func_name_str = resolve_config_val(job['name'], context_vars)
        target_table_str = resolve_config_val(job['target_table'], context_vars)
        key_column = job['key_column']
        key_type_raw = job['key_type']
        key_type = key_type_raw.lower()
        
        batch_size = job.get('batch_size', 1000)
        max_limit = job.get('max_limit', batch_size * 10) 
        filter_columns = job.get('filter_columns', [])
        
        default_val = "0"
        declare_section = ""
        logic_section = ""
        
        func_args_list = [
            f"p_last_{key_column} {key_type_raw} DEFAULT {default_val}",
            "p_limit INTEGER DEFAULT {}".format(batch_size)
        ]
        
        for fc in filter_columns:
            fc_name = fc['name']
            fc_type = fc['type']
            func_args_list.append(f"p_{fc_name} {fc_type} DEFAULT NULL")
        
        func_args_sql = ", ".join(func_args_list)
        
        if 'int' in key_type or 'num' in key_type or 'serial' in key_type:
            default_val = "0"
            declare_section = f"""
                v_limit INTEGER;
                v_start_val BIGINT;
                v_end_val BIGINT;
                v_query TEXT;
                r RECORD;
            """
            logic_section = f"""
                v_limit := LEAST(COALESCE(p_limit, {batch_size}), {max_limit});
                v_start_val := p_last_{key_column};
                v_end_val := p_last_{key_column} + v_limit;
                
                v_query := 'SELECT * FROM {target_table_str} ' ||
                           'WHERE {key_column} > ' || v_start_val::text || 
                           ' AND {key_column} <= ' || v_end_val::text;
            """
            
        elif 'time' in key_type or 'date' in key_type:
            default_val = "'1900-01-01 00:00:00'::timestamp"
            declare_section = f"""
                v_limit INTEGER;
                v_start_val TIMESTAMP;
                v_end_val TIMESTAMP;
                v_query TEXT;
                r RECORD;
            """
            logic_section = f"""
                v_limit := LEAST(COALESCE(p_limit, {batch_size}), {max_limit});
                v_start_val := p_last_{key_column};
                v_end_val := p_last_{key_column} + make_interval(days => v_limit);
                
                v_query := 'SELECT * FROM {target_table_str} ' ||
                           'WHERE {key_column} > ' || quote_literal(v_start_val) || 
                           ' AND {key_column} <= ' || quote_literal(v_end_val);
            """
        else:
            print(f"      âš ï¸ Key type '{key_type}' not supported.")
            continue

        filter_logic_block = []
        for fc in filter_columns:
            fc_name = fc['name']
            fc_type = fc['type']
            fc_type_lower = fc_type.lower()
            
            if 'int' in fc_type_lower or 'num' in fc_type_lower or 'serial' in fc_type_lower:
                filter_logic_block.append(f"""
                IF p_{fc_name} IS NOT NULL THEN
                   v_query := v_query || ' AND {fc_name} = ' || p_{fc_name}::{fc_type}::text;
                END IF;
                """)
            else:
                filter_logic_block.append(f"""
                IF p_{fc_name} IS NOT NULL THEN
                   v_query := v_query || ' AND {fc_name} = ' || quote_literal(p_{fc_name});
                END IF;
                """)

        filter_logic_sql = "\n".join(filter_logic_block)

        execution_block = f"""
            {filter_logic_sql}
            FOR r IN EXECUTE v_query LOOP
                RETURN NEXT r;
            END LOOP;
        """

        func_sql = f"""
        CREATE OR REPLACE FUNCTION {func_name_str}(
            {func_args_sql}
        )
        RETURNS SETOF {target_table_str} 
        LANGUAGE plpgsql 
        SECURITY DEFINER 
        AS $$
        DECLARE
            {declare_section}
        BEGIN
            {logic_section}
            {execution_block}
            RETURN;
        END;
        $$;
        """
        
        try:
            cur.execute(func_sql)
            print(f"      âš™ï¸  Created Incremental Function: {func_name_str}")
        except Exception as e:
            print(f"      âŒ Function Error: {e}")
    cur.close()

# ==========================================
# ØªÙˆØ§Ø¨Ø¹ Ø³ÙØ§Ø±Ø´ÛŒ (Custom Functions)
# ==========================================

def create_custom_functions(conn_db, functions_config, context_vars):
    cur = conn_db.cursor()
    for func in functions_config:
        func_name_str = resolve_config_val(func['name'], context_vars)
        
        # Ø­Ø§Ù„Øª Û±: ØªØ¹Ø±ÛŒÙ Ø¨Ø§ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù† Ùˆ SQL Ø®Ø§Ù…
        if 'arguments' in func or ('returns' in func and 'sql' in func and 'target_table' not in func):
            args_list = []
            for arg in func.get('arguments', []):
                args_list.append(f"{arg['name']} {arg['type']}")
            args_sql = ", ".join(args_list)
            
            ret_type = func.get('returns', 'void')
            lang = func.get('language', 'plpgsql')
            body = func['sql']
            body_resolved = resolve_config_val(body, context_vars)
            
            func_sql = f"""
            CREATE OR REPLACE FUNCTION {func_name_str}({args_sql})
            RETURNS {ret_type}
            LANGUAGE {lang}
            AS $$
            {body_resolved}
            $$;
            """
            
            try:
                cur.execute(func_sql)
                print(f"      âš™ï¸  Created Structured Function: {func_name_str}")
            except Exception as e:
                print(f"      âŒ Structured Function Error {func_name_str}: {e}")
            continue

        # Ø­Ø§Ù„Øª Û²: Raw SQL
        if 'sql' in func:
            raw_sql = func['sql']
            raw_sql_resolved = resolve_config_val(raw_sql, context_vars)
            try:
                cur.execute(sql.SQL(raw_sql_resolved))
                print(f"      âš™ï¸  Created Raw SQL Function: {func_name_str}")
            except Exception as e:
                print(f"      âŒ Raw SQL Function Error {func_name_str}: {e}")
            continue

        # Ø­Ø§Ù„Øª Û³: ØªÙˆÙ„ÛŒØ¯ Ø®ÙˆØ¯Ú©Ø§Ø± (Static SQL - Optimized for FDW)
        target_table_str = resolve_config_val(func['target_table'], context_vars)
        
        return_columns = func.get('return_columns', [])
        filter_columns = func.get('filter_columns', [])
        
        if not return_columns:
            print(f"      âš ï¸ Skipping {func_name_str}: definition unclear.")
            continue
            
        ret_def_list = [f"{rc['name']} {rc['type']}" for rc in return_columns]
        ret_def_sql = ", ".join(ret_def_list)
        
        select_expr_list = [f"{rc['expression']} AS {rc['name']}" for rc in return_columns]
        select_expr_sql = ", ".join(select_expr_list)
        
        args_list = [f"p_{fc['name']} {fc['type']} DEFAULT NULL" for fc in filter_columns]
        args_sql = ", ".join(args_list)
        
        # ØªÙˆÙ„ÛŒØ¯ Ø´Ø±Ø·â€ŒÙ‡Ø§ÛŒ WHERE (Static SQL)
        where_parts = []
        for fc in filter_columns:
            fc_name = fc['name']
            fc_op = fc.get('filter_type', '=')
            
            # Ø´Ø±Ø· Ø§Ù…Ù†: Ø§Ú¯Ø± ÙˆØ±ÙˆØ¯ÛŒ NULL Ø¨Ø§Ø´Ø¯ØŒ Ø´Ø±Ø· Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
            cond = f"({fc_name} {fc_op} p_{fc_name} OR p_{fc_name} IS NULL)"
            where_parts.append(cond)
            
        where_clause = " AND ".join(where_parts)
        if not where_clause:
            where_clause = "TRUE"

        drop_types_list = [fc['type'] for fc in filter_columns]
        drop_types_sql = ", ".join(drop_types_list)
        drop_sql = f"DROP FUNCTION IF EXISTS {func_name_str}({drop_types_sql});"
        
        func_sql = f"""
        {drop_sql}
        
        CREATE OR REPLACE FUNCTION {func_name_str}({args_sql})
        RETURNS TABLE({ret_def_sql})
        LANGUAGE plpgsql
        SECURITY DEFINER
        AS $$
        BEGIN
            RETURN QUERY
            SELECT {select_expr_sql} 
            FROM {target_table_str} 
            WHERE {where_clause};
        END;
        $$;
        """
        
        try:
            cur.execute(func_sql)
            print(f"      âš™ï¸  Created Generated Function (Static): {func_name_str}")
        except Exception as e:
            print(f"      âŒ Function Error {func_name_str}: {e}")
    cur.close()

# ==========================================
# Ù…Ù†Ø·Ù‚ Ø§ØµÙ„ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´
# ==========================================

def create_global_users(all_users_list):
    if not all_users_list: return
    print("ğŸ‘¥ Creating Global Users (Roles)...")
    conn_glob = connect(DB_DEFAULT_NAME)
    cur = conn_glob.cursor()
    created_set = set()
    for user_data in all_users_list:
        username = resolve_global_env(user_data.get('username'))
        password = resolve_global_env(user_data.get('password'))
        if not username or not password: continue
        if username in created_set: continue
        try:
            cur.execute(sql.SQL("CREATE USER {} WITH PASSWORD %s;").format(sql.Identifier(username)), (password,))
            print(f"   âœ… Created user: {username}")
            created_set.add(username)
        except psycopg2.errors.DuplicateObject:
            try:
                cur.execute(sql.SQL("ALTER USER {} WITH PASSWORD %s;").format(sql.Identifier(username)), (password,))
            except: pass
            created_set.add(username)
        except Exception as e:
            print(f"   âŒ User creation error ({username}): {e}")
    conn_glob.close()

def scan_and_collect_users(config_dir):
    users_list = []
    for folder_name in ['users', 'user']:
        for u_file in config_dir.rglob(f"{folder_name}/*.yaml"):
            try:
                with open(u_file, 'r') as f:
                    user_cfg = yaml.safe_load(f)
                    if user_cfg and (user_cfg.get('username') or user_cfg.get('name')):
                        users_list.append({
                            'username': resolve_global_env(user_cfg.get('username', user_cfg.get('name'))),
                            'password': resolve_global_env(user_cfg.get('password'))
                        })
            except: pass
    return users_list

def process_single_database(db_name_resolved, db_data, context_vars):
    print(f"\n   {'='*15} DATABASE: {db_name_resolved} {'='*15}")
    
    conn_admin = connect(DB_DEFAULT_NAME)
    db_owner = DB_ADMIN_USER
    for cfg in db_data['configs']:
        if 'database' in cfg and 'owner' in cfg['database']:
            db_owner = resolve_config_val(cfg['database']['owner'], context_vars)
            break

    try:
        conn_admin.cursor().execute(sql.SQL("CREATE DATABASE {} OWNER {};").format(sql.Identifier(db_name_resolved), sql.Identifier(db_owner)))
        print("      âœ… Database created.")
    except psycopg2.errors.DuplicateDatabase:
        print("      â„¹ï¸  Database exists.")
    except Exception as e:
        print(f"      âŒ DB Error: {e}")
        conn_admin.close()
        return
    finally:
        conn_admin.close()

    conn_db = connect(db_name_resolved)
    cur = conn_db.cursor()

    for ext in ['mysql_fdw', 'tds_fdw', 'pg_stat_statements']:
        try: cur.execute(sql.SQL("CREATE EXTENSION IF NOT EXISTS {};").format(sql.Identifier(ext)))
        except: pass
    
    setup_security_infrastructure(conn_db)

    all_schemas = []
    all_fdws = []
    all_tables = []
    all_views = []
    all_jobs = []
    all_custom_funcs = []
    all_permissions = []
    
    schema_names_set = set()
    fdw_names_set = set()
    table_names_set = set()
    view_names_set = set()
    job_names_set = set()
    custom_func_names_set = set()

    for cfg in db_data['configs']:
        for s in cfg.get('schemas', []):
            s_name = resolve_config_val(s['name'], context_vars)
            if s_name not in schema_names_set:
                all_schemas.append(s); schema_names_set.add(s_name)
        
        for f in cfg.get('fdws', []):
            f_name = resolve_config_val(f['name'], context_vars)
            if f_name not in fdw_names_set:
                all_fdws.append(f); fdw_names_set.add(f_name)
                
        for t in cfg.get('tables', []):
            t_name = resolve_config_val(t['name'], context_vars)
            if t_name not in table_names_set:
                all_tables.append(t); table_names_set.add(t_name)
                
        for v in cfg.get('views', []):
            v_name = resolve_config_val(v['name'], context_vars)
            if v_name not in view_names_set:
                all_views.append(v); view_names_set.add(v_name)
                
        for j in cfg.get('incremental_jobs', []):
            j_name = resolve_config_val(j['name'], context_vars)
            if j_name not in job_names_set:
                all_jobs.append(j); job_names_set.add(j_name)
        
        for cf in cfg.get('custom_functions', []):
            cf_name = resolve_config_val(cf['name'], context_vars)
            if cf_name not in custom_func_names_set:
                all_custom_funcs.append(cf); custom_func_names_set.add(cf_name)

    for perm_cfg in db_data['user_permissions']:
        all_permissions.append(perm_cfg)

    print("      ğŸ—ï¸  Schemas...")
    for s in all_schemas:
        s_name = resolve_config_val(s['name'], context_vars)
        cur.execute(sql.SQL("CREATE SCHEMA IF NOT EXISTS {};").format(sql.Identifier(s_name)))

    print("      ğŸŒ FDWs...")
    fdw_credentials = {}
    
    for fdw in all_fdws:
        fdw_name = resolve_config_val(fdw['name'], context_vars)
        fdw_type = fdw['type']
        options = {}
        
        if fdw_type == 'tds':
            if 'config_section' in fdw:
                options['servername'] = fdw['config_section']
                if 'database' in fdw: options['database'] = resolve_config_val(fdw['database'], context_vars)
            else:
                host_val = resolve_config_val(fdw.get('host'), context_vars)
                options['servername'] = host_val
                options['port'] = str(resolve_config_val(fdw.get('port'), context_vars))
                if 'database' in fdw: options['database'] = resolve_config_val(fdw['database'], context_vars)
            options['tds_version'] = fdw.get('tds_version', '7.4')

        elif fdw_type == 'mysql':
            options['host'] = resolve_config_val(fdw.get('host'), context_vars)
            options['port'] = str(resolve_config_val(fdw.get('port'), context_vars))
            if 'database' in fdw: options['dbname'] = resolve_config_val(fdw['database'], context_vars)

        opts_sql = sql.SQL(', ').join([sql.SQL("{} {}").format(sql.Identifier(k), sql.Literal(v)) for k,v in options.items() if v])
        
        try: 
            cur.execute(sql.SQL("CREATE SERVER {} FOREIGN DATA WRAPPER {} OPTIONS ({});").format(
                sql.Identifier(fdw_name), sql.Identifier(f"{fdw_type}_fdw"), opts_sql))
        except psycopg2.errors.DuplicateObject: 
            pass
        
        fdw_credentials[fdw_name] = {
            'user': resolve_config_val(fdw['user'], context_vars),
            'password': resolve_config_val(fdw['password'], context_vars)
        }

    print("      ğŸ“Š Tables...")
    for tbl in all_tables:
        tbl_name = resolve_config_val(tbl['name'], context_vars)
        server_name = resolve_config_val(tbl['server'], context_vars)
        cols = sql.SQL(', ').join([sql.SQL("{} {}").format(sql.Identifier(c['name']), sql.SQL(c['type'])) for c in tbl['columns']])
        
        opts = []
        if 'remote_schema' in tbl: opts.append(sql.SQL("schema_name {}").format(sql.Literal(tbl['remote_schema'])))
        if 'remote_table' in tbl: opts.append(sql.SQL("table_name {}").format(sql.Literal(tbl['remote_table'])))
        opts_sql = sql.SQL(', ').join(opts)
        
        try: 
            cur.execute(sql.SQL("CREATE FOREIGN TABLE {} ({}) SERVER {} OPTIONS ({});").format(parse_identifier(tbl_name), cols, sql.Identifier(server_name), opts_sql))
            print(f"         âœ… Created table: {tbl_name}")
        except psycopg2.errors.DuplicateTable: 
            print(f"         â„¹ï¸  Table exists: {tbl_name}")
        except Exception as e: 
            print(f"      âŒ Table Error {tbl_name}: {e}")

    print("      ğŸ‘ï¸  Views...")
    for vw in all_views:
        vw_name = resolve_config_val(vw['name'], context_vars)
        vw_sql = resolve_config_val(vw['sql'], context_vars)
        try: 
            cur.execute(sql.SQL("CREATE OR REPLACE VIEW {} AS {};").format(parse_identifier(vw_name), sql.SQL(vw_sql)))
        except Exception as e: 
            print(f"      âŒ View Error {vw_name}: {e}")

    if all_jobs:
        create_incremental_job_functions(conn_db, all_jobs, context_vars)
    
    if all_custom_funcs:
        create_custom_functions(conn_db, all_custom_funcs, context_vars)

    print("      ğŸ” Permissions...")
    
    users_in_this_db = []

    for user in all_permissions:
        username = resolve_config_val(user.get('username'), context_vars)
        users_in_this_db.append(username)

        cur.execute(sql.SQL("GRANT CONNECT ON DATABASE {} TO {};").format(sql.Identifier(db_name_resolved), sql.Identifier(username)))
        
        for s_name in schema_names_set:
            cur.execute(sql.SQL("GRANT USAGE ON SCHEMA {} TO {};").format(sql.Identifier(s_name), sql.Identifier(username)))

        access_time = user.get('access_time')
        if access_time: 
            st = access_time.get('start')
            en = access_time.get('end')
            if st and en: 
                cur.execute("INSERT INTO auth_policies (username, allowed_start, allowed_end) VALUES (%s,%s,%s) ON CONFLICT (username) DO UPDATE SET allowed_start=%s, allowed_end=%s;", 
                           (username, st, en, st, en))

        for perm in user.get('permissions', []):
            if 'view' in perm:
                v_conf = perm['view']
                v_name = None
                cols = None
                if isinstance(v_conf, str): v_name = resolve_config_val(v_conf, context_vars)
                elif isinstance(v_conf, dict):
                    v_name = resolve_config_val(v_conf.get('name'), context_vars)
                    cols = v_conf.get('columns')
                if v_name:
                    try:
                        if cols:
                            cols_sql = sql.SQL(', ').join([sql.Identifier(c) for c in cols])
                            cur.execute(sql.SQL("GRANT SELECT ({}) ON {} TO {};").format(cols_sql, parse_identifier(v_name), sql.Identifier(username)))
                        else:
                            cur.execute(sql.SQL("GRANT SELECT ON {} TO {};").format(parse_identifier(v_name), sql.Identifier(username)))
                    except: pass
            if 'table' in perm:
                t_conf = perm['table']
                t_name = None
                if isinstance(t_conf, str): t_name = resolve_config_val(t_conf, context_vars)
                elif isinstance(t_conf, dict): t_name = resolve_config_val(t_conf.get('name'), context_vars)
                if t_name:
                    try: cur.execute(sql.SQL("GRANT SELECT ON {} TO {};").format(parse_identifier(t_name), sql.Identifier(username)))
                    except: pass
            if 'function' in perm:
                f_conf = perm['function']
                f_name = None
                if isinstance(f_conf, str): f_name = resolve_config_val(f_conf, context_vars)
                elif isinstance(f_conf, dict): f_name = resolve_config_val(f_conf.get('name'), context_vars)
                if f_name:
                    try: cur.execute(sql.SQL("GRANT EXECUTE ON FUNCTION {} TO {};").format(parse_identifier(f_name), sql.Identifier(username)))
                    except: pass

    for job in all_jobs:
        func_name_str = resolve_config_val(job['name'], context_vars)
        for consumer in job.get('allowed_consumers', []):
            resolved_user = resolve_config_val(consumer, context_vars)
            try: cur.execute(sql.SQL("GRANT EXECUTE ON FUNCTION {} TO {};").format(parse_identifier(func_name_str), sql.Identifier(resolved_user)))
            except: pass
            
    for cf in all_custom_funcs:
        func_name_str = resolve_config_val(cf['name'], context_vars)
        for consumer in cf.get('allowed_consumers', []):
            resolved_user = resolve_config_val(consumer, context_vars)
            try: cur.execute(sql.SQL("GRANT EXECUTE ON FUNCTION {} TO {};").format(parse_identifier(func_name_str), sql.Identifier(resolved_user)))
            except: pass

    print("      ğŸ”— Creating User Mappings...")
    for fdw_name in fdw_credentials:
        try: 
            cur.execute(sql.SQL("CREATE USER MAPPING FOR {} SERVER {} OPTIONS (username %s, password %s);").format(
                sql.Identifier(DB_ADMIN_USER), sql.Identifier(fdw_name)), 
                (fdw_credentials[fdw_name]['user'], fdw_credentials[fdw_name]['password']))
        except psycopg2.errors.DuplicateObject: 
            pass

        for l_user in users_in_this_db:
            try: 
                cur.execute(sql.SQL("CREATE USER MAPPING FOR {} SERVER {} OPTIONS (username %s, password %s);").format(
                    sql.Identifier(l_user), sql.Identifier(fdw_name)), 
                    (fdw_credentials[fdw_name]['user'], fdw_credentials[fdw_name]['password']))
            except: pass

    conn_db.close()
    print(f"      âœ… Database {db_name_resolved} Processing Complete.")

def process_domain(domain_path):
    parent_dir = domain_path.parent.name
    current_dir = domain_path.name
    context_vars = {'__parent__': parent_dir, '__current__': current_dir}
    print(f"\n{'='*20} PROCESSING DOMAIN: {parent_dir}/{current_dir} {'='*20}")
    db_configs_map = defaultdict(lambda: {"configs": [], "user_permissions": []})
    for y_file in domain_path.glob("*.yaml"):
        try:
            with open(y_file, 'r') as f:
                cfg = yaml.safe_load(f)
                if not cfg: continue
                if 'database' in cfg:
                    db_name = resolve_config_val(cfg['database']['name'], context_vars)
                    db_configs_map[db_name]['configs'].append(cfg)
        except Exception as e:
            print(f"      âŒ Error reading {y_file.name}: {e}")
    users_path = domain_path / "users"
    if not users_path.exists(): users_path = domain_path / "user"
    if users_path.exists():
        for u_file in users_path.glob("*.yaml"):
            try:
                with open(u_file, 'r') as f:
                    user_cfg = yaml.safe_load(f)
                    if not user_cfg: continue
                    for grant in user_cfg.get('grants', []):
                        target_db = resolve_config_val(grant.get('database'), context_vars)
                        if target_db:
                            user_data = {
                                'username': resolve_config_val(user_cfg.get('username', user_cfg.get('name')), context_vars),
                                'access_time': grant.get('access_time', user_cfg.get('access_time')),
                                'permissions': grant.get('permissions', [])
                            }
                            db_configs_map[target_db]['user_permissions'].append(user_data)
            except Exception as e:
                print(f"      âŒ Error reading user file {u_file.name}: {e}")
    for db_name, data in db_configs_map.items():
        process_single_database(db_name, data, context_vars)

def main():
    print("ğŸš€ Starting Modular Provisioning Engine...")
    config_dir = Path("/app/configs/domains")
    all_users = scan_and_collect_users(config_dir)
    create_global_users(all_users)
    if config_dir.exists():
        for project_folder in config_dir.iterdir():
            if project_folder.is_dir():
                has_subfolders = any(f.is_dir() for f in project_folder.iterdir() if f.name not in ['user', 'users'])
                if has_subfolders:
                    for domain_folder in project_folder.iterdir():
                        if domain_folder.is_dir() and domain_folder.name not in ['user', 'users']:
                            process_domain(domain_folder)
                else:
                    process_domain(project_folder)
    print("\nğŸ‰ ALL TASKS COMPLETED.")

if __name__ == "__main__": main()
```

### Û¶. ÙØ§ÛŒÙ„ Docker Compose

**ÙØ§ÛŒÙ„: `docker-compose.yml`**

```yaml
services:
  postgres:
    build:
      context: .
      dockerfile: ./docker/Dockerfile
      
    container_name: ${CONTAINER_NAME}
    hostname: ${CONTAINER_HOSTNAME}
    env_file:
      - .env
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - TZ=Asia/Tehran
      - BACKUP_DIR=${BACKUP_DIR}
      - BACKUP_RETENTION_DAYS=${BACKUP_RETENTION_DAYS}

    ports:
      - "${POSTGRES_PORT}:5432"
    
    volumes:
      - db_gateway_data:/var/lib/postgresql/18/data
      - ./configs:/app/configs
      - ./scripts:/app/scripts
      - ./${BACKUP_DIR}:/app/${BACKUP_DIR}
      - ./.env:/app/.env:ro
      - ./configs/freetds.conf:/etc/freetds.conf:ro
    
    command: ["/app/scripts/init-db.sh"]
    
    networks:
      - db-gateway-network
    restart: always

  pgbouncer:
    image: edoburu/pgbouncer:latest
    container_name: pg_pgbouncer
    environment:
      DB_HOST: postgres
      DB_PORT: 5432
      DB_USER: ${POSTGRES_USER}
      DB_PASSWORD: ${POSTGRES_PASSWORD}
      DATABASES: gateway_db
      POOL_MODE: transaction
      AUTH_TYPE: md5
    ports:
      - "6432:5432"
    depends_on:
      - postgres
    networks:
      - db-gateway-network
    restart: always

volumes:
  db_gateway_data:

networks:
  db-gateway-network:
    driver: bridge
```

---

## Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ
Ø§ÛŒÙ† Ø³Ù†Ø¯ Ùˆ Ú©Ø¯Ù‡Ø§ÛŒ Ù‡Ù…Ø±Ø§Ù‡ Ø¢Ù†ØŒ ÛŒÚ© Ø±Ø§Ù‡â€ŒØ­Ù„ Ø³Ø·Ø­ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ (Enterprise-Grade) Ø¨Ø±Ø§ÛŒ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯. Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ÛŒÙ† Ø³Ø§Ø®ØªØ§Ø±:
1.  ØªÙˆØ³Ø¹Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ú¯Ø§Ù† ÙÙ‚Ø· Ø¨Ø§ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ YAML Ø³Ø± Ùˆ Ú©Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯.
2.  Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ FDW Ùˆ PL/pgSQL Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ø¯ÛŒØ±ÛŒØª Ù…ÛŒâ€ŒØ´ÙˆØ¯.
3.  Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³ÛŒØ³ØªÙ… Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Static SQL Ùˆ Connection Pooling (PgBouncer) ØªØ¶Ù…ÛŒÙ† Ù…ÛŒâ€ŒØ´ÙˆØ¯.